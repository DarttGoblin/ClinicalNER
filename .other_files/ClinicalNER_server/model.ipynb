{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3459c8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers datasets seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, load_metric\n",
    "import pandas as pd\n",
    "\n",
    "def read_bio_tsv(filename):\n",
    "    sentences, labels = [], []\n",
    "    tokens, tags = [], []\n",
    "\n",
    "    with open(filename, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"\":\n",
    "                if tokens:\n",
    "                    sentences.append(tokens)\n",
    "                    labels.append(tags)\n",
    "                    tokens, tags = [], []\n",
    "            else:\n",
    "                splits = line.split(\"\\t\")\n",
    "                if len(splits) == 2:\n",
    "                    tokens.append(splits[0])\n",
    "                    tags.append(splits[1])\n",
    "    return sentences, labels\n",
    "\n",
    "sentences, ner_tags = read_bio_tsv(\"biobert_ner_data.tsv\")\n",
    "\n",
    "df = pd.DataFrame({\"tokens\": sentences, \"ner_tags\": ner_tags})\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "dataset = DatasetDict({\"train\": dataset[\"train\"], \"test\": dataset[\"test\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b01e8d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "unique_tags = sorted(set(tag for tags in ner_tags for tag in tags))\n",
    "tag2id = {tag: i for i, tag in enumerate(unique_tags)}\n",
    "id2tag = {i: tag for tag, i in tag2id.items()}\n",
    "\n",
    "def encode_labels(example):\n",
    "    example[\"labels\"] = [tag2id[tag] for tag in example[\"ner_tags\"]]\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(encode_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_checkpoint = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(tag2id),\n",
    "    id2label=id2tag,\n",
    "    label2id=tag2id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized = tokenizer(example[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    word_ids = tokenized.word_ids()\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            labels.append(example[\"labels\"][word_idx])\n",
    "        else:\n",
    "            label = example[\"labels\"][word_idx]\n",
    "            if id2tag[label].startswith(\"B-\"):\n",
    "                label = tag2id[\"I-\" + id2tag[label][2:]]\n",
    "            labels.append(label)\n",
    "        previous_word_idx = word_idx\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds, labels = p\n",
    "    preds = preds.argmax(-1)\n",
    "    true_preds, true_labels = [], []\n",
    "    for pred, label in zip(preds, labels):\n",
    "        true_pred = [id2tag[p] for (p, l) in zip(pred, label) if l != -100]\n",
    "        true_label = [id2tag[l] for (p, l) in zip(pred, label) if l != -100]\n",
    "        true_preds.append(true_pred)\n",
    "        true_labels.append(true_label)\n",
    "    return seqeval.compute(predictions=true_preds, references=true_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./biobert-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"./biobert-ner-final\")\n",
    "tokenizer.save_pretrained(\"./biobert-ner-final\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
